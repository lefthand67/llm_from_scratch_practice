{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa725d5-2957-4536-8ec8-82ed546d6b03",
   "metadata": {},
   "source": [
    "# FOUNDATION F: NEURAL NETWORKS FROM SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b605a9-7d78-4433-888b-8d2da4e22624",
   "metadata": {},
   "source": [
    "# <b>F.1 Gradient</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca004a6-7e46-40ac-b5a6-761a330dfa83",
   "metadata": {},
   "source": [
    "**The Core Idea**: A neural network is just a mathematical function that can be represented as a computational graph. The \"learning\" happens by adjusting the parameters of this function to minimize some error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b34ae0-059b-4d9e-9f08-a5e446df7738",
   "metadata": {},
   "source": [
    "Think about a single neuron - the simplest building block. If you were to implement this from scratch in NumPy (no PyTorch yet), what would be the minimal components you'd need?\n",
    "\n",
    "Consider:\n",
    "- What inputs does it take?\n",
    "- What parameters does it have? \n",
    "- What computation does it perform?\n",
    "- What output does it produce?\n",
    "\n",
    "1. **Linear combination**: $y = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b$\n",
    "2. **Weights initialization**: Random values\n",
    "3. **Activation function**: Breaks linearity\n",
    "4. **Matrix operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3792ba-fda5-4d5d-bc6b-bab4ae81910f",
   "metadata": {},
   "source": [
    "## The Linear Algebra Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7f8ef-17c0-4c30-abe3-5a31ec3f4489",
   "metadata": {},
   "source": [
    "Think about processing one input vector vs many inputs:\n",
    "\n",
    "**Single input**: $[x_1, x_2, x_3]$ with weights $[\\theta_1, \\theta_2, \\theta_3]$  \n",
    "$output = x_1\\theta_1 + x_2\\theta_2 + x_3\\theta_3 + b$\n",
    "\n",
    "**Multiple inputs (as matrix)**: \n",
    "```\n",
    "Inputs: [ [x‚ÇÅ‚ÇÅ, x‚ÇÅ‚ÇÇ, x‚ÇÅ‚ÇÉ],   Weights: [ùúÉ‚ÇÅ, ùúÉ‚ÇÇ, ùúÉ‚ÇÉ]·µÄ\n",
    "          [x‚ÇÇ‚ÇÅ, x‚ÇÇ‚ÇÇ, x‚ÇÇ‚ÇÉ],\n",
    "          [x‚ÇÉ‚ÇÅ, x‚ÇÉ‚ÇÇ, x‚ÇÉ‚ÇÉ] ]\n",
    "```\n",
    "\n",
    "What linear algebra operation would efficiently compute all outputs at once? Matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b372b9-f12c-40b2-b2ec-8a53e3f8cfd5",
   "metadata": {},
   "source": [
    "## Single neuron code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28bba8-a3a1-4c24-a25b-04a971ea77dd",
   "metadata": {},
   "source": [
    "Try implementing the **single input case** first:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize weights and bias here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute y = w¬∑x + b\n",
    "        # Then apply activation function\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772379a-5a41-4cdc-856b-6c0e9e7dfd6d",
   "metadata": {},
   "source": [
    "## Activation Function Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2b2f0-d6ad-45c5-a8be-8861abf56c4b",
   "metadata": {},
   "source": [
    "| Function | Formula | Range | Key Properties |\n",
    "|----------|---------|-------|----------------|\n",
    "| **Sigmoid** | 1/(1+e‚ÅªÀ£) | (0,1) | Smooth, bounded, but can saturate (vanishing gradients) |\n",
    "| **Tanh** | (eÀ£-e‚ÅªÀ£)/(eÀ£+e‚ÅªÀ£) | (-1,1) | Zero-centered, but still can saturate |\n",
    "| **ReLU** | max(0,x) | [0,‚àû) | Simple, avoids saturation, but \"dying ReLU\" problem |\n",
    "| **Leaky ReLU** | max(0.01x,x) | (-‚àû,‚àû) | Fixes dying ReLU, small gradient for negatives |\n",
    "\n",
    "**Historical Context & Modern Practice**\n",
    "\n",
    "- **1980s-2000s**: Sigmoid/tanh were dominant (biological plausibility)\n",
    "- **2010s**: ReLU became standard for hidden layers (training speed)\n",
    "- **Today**: Variants like Leaky ReLU, GELU are common\n",
    "\n",
    "For your **first from-scratch implementation**, which activation function would you choose and **why**? Consider:\n",
    "\n",
    "1. **Simplicity of implementation**\n",
    "2. **Gradient behavior** (important for backpropagation)\n",
    "3. **Training stability**\n",
    "\n",
    "\n",
    "\n",
    "Leaky ReLU is actually a very smart pick for your first implementation.\n",
    "\n",
    "**Why Leaky ReLU is a Great Choice**\n",
    "\n",
    "1. **Simple to implement** - just `max(0.01*x, x)` \n",
    "2. **Avoids the \"dying neuron\" problem** of regular ReLU\n",
    "3. **Computationally efficient** - no expensive exponentials like sigmoid/tanh\n",
    "4. **Good gradient flow** - never completely zero gradient\n",
    "\n",
    "**Your Implementation Challenge**\n",
    "\n",
    "Now, here's what I want you to think about for the implementation:\n",
    "\n",
    "```python\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    # Your implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Key consideration**: Should this be a method of your `Neuron` class, or a separate function? What are the trade-offs?\n",
    "\n",
    "Also think about: How will this affect your backpropagation implementation later? (We'll get to that soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423ddfb-c9b4-4254-aa5e-c9c52f46d4c9",
   "metadata": {},
   "source": [
    "Putting the activation function in the forward pass is exactly right.\n",
    "\n",
    "**Your Architectural Decision Makes Sense**\n",
    "\n",
    "Having the activation as part of the neuron's forward pass:\n",
    "- Keeps the neuron self-contained\n",
    "- Makes the computation clear: `output = activation(linear_combination)`\n",
    "- Will make backpropagation more straightforward (when we get there)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23676f4f-e2cb-42a1-bac6-c223ec807387",
   "metadata": {},
   "source": [
    "### Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c44b2369-d4ca-424d-a998-e8b79d9bef41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:02:08.298334Z",
     "iopub.status.busy": "2025-11-09T10:02:08.294394Z",
     "iopub.status.idle": "2025-11-09T10:02:08.428169Z",
     "shell.execute_reply": "2025-11-09T10:02:08.426235Z",
     "shell.execute_reply.started": "2025-11-09T10:02:08.298028Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"Implement single neuron class.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: tuple) -> None:\n",
    "        \"\"\"Initialize neuron entity.\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.w = np.random.default_rng().random(self.input_size[-1])\n",
    "        self.b = np.random.default_rng().random()\n",
    "        # activation function hyperparameter\n",
    "        self.alpha = 0.01\n",
    "\n",
    "    def leaky_relu(self, vector: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Break linearity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector : np.array\n",
    "            y_pred before activation function applied.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.maximum(self.alpha * vector, vector)\n",
    "\n",
    "    def _get_linear_transformation(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make the matrix multiplication of x and weights.\n",
    "\n",
    "        The result is y_pred before activation function.\n",
    "        \"\"\"\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"Calculate forward pass with activation function.\"\"\"\n",
    "        vector = self._get_linear_transformation(x)\n",
    "        return self.leaky_relu(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44f62454-e623-4b52-bedf-3e09d403e38b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:02:48.032181Z",
     "iopub.status.busy": "2025-11-09T10:02:48.031252Z",
     "iopub.status.idle": "2025-11-09T10:02:48.053346Z",
     "shell.execute_reply": "2025-11-09T10:02:48.051971Z",
     "shell.execute_reply.started": "2025-11-09T10:02:48.032041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_neuron.input_size: (4, 5)\n",
      "my_neuron.w: [0.96963449 0.88127309 0.29762979 0.32335681 0.75624471]\n",
      "linear transformation: [19.55678708 20.20853882 31.18914713 -5.57078139]\n",
      "my_neuron.forward(X): [19.55678708 20.20853882 31.18914713 -0.05570781]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [-1, 2, 3, 13, 17],\n",
    "        [4, -5, 6, 14, 18],\n",
    "        [7, 8, -9, 15, 19],\n",
    "        [-10, 11, 12, 16, -20],\n",
    "    ],\n",
    "    dtype=np.float64,\n",
    ")\n",
    "\n",
    "my_neuron = Neuron((X.shape))\n",
    "print(\"my_neuron.input_size:\", my_neuron.input_size)\n",
    "print(\"my_neuron.w:\", my_neuron.w)\n",
    "print(\"linear transformation:\", my_neuron._get_linear_transformation(X))\n",
    "print(\"my_neuron.forward(X):\", my_neuron.forward(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b044b-8c62-4be7-b29b-cb111c7ba50b",
   "metadata": {},
   "source": [
    "## Gradient derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00178c04-038b-44b4-991d-c2be8aabaccf",
   "metadata": {},
   "source": [
    "Your single neuron implementation is solid. The next critical concept is: **how does learning actually happen?**\n",
    "\n",
    "We have this neuron that can compute outputs, but how do we adjust `self.w` and `self.b` to make it produce better outputs?\n",
    "\n",
    "What's your current understanding of how the \"learning\" process works in neural networks?\n",
    "\n",
    "You've identified the three key components:\n",
    "\n",
    "1. **Error/Loss**\n",
    "2. **Gradient**\n",
    "3. **Weight Update**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68fe6d-16c5-4c20-9d18-3336c2defe09",
   "metadata": {},
   "source": [
    "### The Missing Piece: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65338f-3f5c-4ee2-b33a-e3778f2bc80c",
   "metadata": {},
   "source": [
    "What you described is **gradient descent**. The magic that makes it work for neural networks is **backpropagation** - the algorithm for efficiently computing those gradients through the entire computational graph.\n",
    "\n",
    "Think of it this way: If your neuron's computation is:\n",
    "```\n",
    "input ‚Üí linear ‚Üí activation ‚Üí output\n",
    "```\n",
    "\n",
    "Backpropagation answers:\n",
    "\n",
    "> \"*How much did each weight contribute to the final error?*\"\n",
    "\n",
    "Let's break this down into manageable steps. If you were to implement gradient calculation for your single neuron, what would be the minimal information you'd need?\n",
    "\n",
    "Consider:\n",
    "- The error at the output\n",
    "- How the activation function affects the gradient\n",
    "- How the linear combination affects the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339eab2-5b5b-4131-b214-d6326450b767",
   "metadata": {},
   "source": [
    "### The Error Signal Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa634e-af90-46e6-bfe4-6ee84cad1d4f",
   "metadata": {},
   "source": [
    "The error signal begins with the difference between predicted and true values.\n",
    "\n",
    "You're correct: `error = true_y - predicted_y` (or some function of this difference)\n",
    "\n",
    "Now here's the key insight for backpropagation: **We need to trace how this error flows backward through each operation.**\n",
    "\n",
    "Think about your neuron's forward pass:\n",
    "```\n",
    "x ‚Üí linear (w¬∑x + b) ‚Üí leaky_relu ‚Üí y_pred\n",
    "```\n",
    "\n",
    "For backpropagation, we go backward:\n",
    "```\n",
    "error ‚Üê d_leaky_relu ‚Üê d_linear ‚Üê gradients for w and b\n",
    "```\n",
    "\n",
    "If you were to implement this gradient calculation manually for a single input, what operations would you need to compute?\n",
    "\n",
    "Consider:\n",
    "1. Gradient of the error with respect to the output\n",
    "2. How the activation function transforms this gradient\n",
    "3. How the linear operation distributes gradients to weights and bias\n",
    "\n",
    "**What's your thinking about the mathematical operations involved?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ba838-8872-4371-a8ba-717b562a4232",
   "metadata": {},
   "source": [
    "### Chain Rule Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94c36f-224e-4864-a266-0f0716d11604",
   "metadata": {},
   "source": [
    "Think about your neuron's computation:\n",
    "```\n",
    "output = leaky_relu(w¬∑x + b)\n",
    "error = (true_y - output)¬≤\n",
    "```\n",
    "\n",
    "If we want to know how much to change `w‚ÇÅ`, we need to answer: \"How does changing `w‚ÇÅ` affect the final error?\"\n",
    "\n",
    "This is where the **chain rule** from calculus comes in. We break the problem into steps:\n",
    "\n",
    "1. How does error change with output?\n",
    "2. How does output change with activation input?\n",
    "3. How does activation input change with `w‚ÇÅ`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef07f8b-494a-4246-8698-c2631fa79d92",
   "metadata": {},
   "source": [
    "You've done exactly the right preparation. Re-reading the math and connecting it to CS229 shows excellent learning discipline.\n",
    "\n",
    "Your understanding is spot on:\n",
    "\n",
    "- **J(Œ∏)** is our cost function (like MSE)\n",
    "- **Partial derivatives** give us the gradient ‚àáJ(Œ∏)  \n",
    "- **Gradient descent** uses this to update weights: Œ∏ = Œ∏ - Œ±‚àáJ(Œ∏)\n",
    "\n",
    "You asked: *\"Do we really need to work with the error itself?\"* The answer reveals the elegance of backpropagation:\n",
    "\n",
    "> We don't actually need the explicit mathematical formula for J(Œ∏). \n",
    "\n",
    "Instead, we use the **chain rule** to compute gradients through the computational graph.\n",
    "\n",
    "Think about your neuron:\n",
    "```\n",
    "x ‚Üí z = ùúÉ¬∑x + b ‚Üí a = leaky_relu(z) ‚Üí J = (y_true - a)¬≤\n",
    "```\n",
    "\n",
    "To find $\\displaystyle \\frac {\\partial}{\\partial \\theta}J(\\theta)$, we can compute:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta}J(\\theta) = \\frac {\\partial J(\\theta)}{\\partial a} \\times \\frac {\\partial a}{\\partial z} \\times \\frac {\\partial z}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "**Your implementation challenge**: If you were to compute these partial derivatives numerically for a single example, what would be your step-by-step approach?\n",
    "\n",
    "(see CS229, Batch Gradient Descent)\n",
    "\\begin{align*}\n",
    "\\frac {\\partial}{\\partial \\theta_j} J(\\theta) & = \\frac {\\partial}{\\partial \\theta_j} \\frac{1}{2} \\left(h_{\\theta}(x) - y\\right)^2 = \\\\\n",
    "& = 2 \\frac {1}{2} (h_{\\theta}(x) - y) \\cdot \\frac {\\partial}{\\partial \\theta_j}(h_{\\theta}(x) - y) = \\\\\n",
    "& = (h_{\\theta}(x) - y) \\cdot \\frac {\\partial}{\\partial \\theta_j} (\\theta_0 x_0 + \\theta_1 x_1 + \\cdots + \\theta_j x_j \\cdots + \\theta_n x_n -y) \\\\\n",
    "& = (h_{\\theta}(x) - y) \\cdot x_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51584f-0f8f-43c7-84bf-ecb9b19b0685",
   "metadata": {},
   "source": [
    "You've derived the gradient for linear regression perfectly! \n",
    "\n",
    "Your derivation:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta_j} J(\\theta) = (h_{\\theta}(x) - y) \\cdot x_j\n",
    "$$\n",
    "\n",
    "The gradient for each weight is proportional to:\n",
    "1. The **error signal** (prediction - true value)\n",
    "2. The **input** that weight connects to\n",
    "\n",
    "Your neuron has one extra step: the activation function. So the chain becomes:\n",
    "\n",
    "For weight $\\theta_j$:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta_j}J(\\theta) = \\frac {\\partial J(\\theta)}{\\partial a} \\times \\frac {\\partial a}{\\partial z} \\times \\frac {\\partial z}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $a = \\text {leaky_relu}(z)$ = final output\n",
    "- $z = \\theta x + b$ = linear combination\n",
    "- $\\displaystyle \\frac {\\partial z}{\\partial \\theta_j} = x_j$ (as you correctly derived)\n",
    "\n",
    "Given your `leaky_relu` function:\n",
    "```python\n",
    "def leaky_relu(self, vector, alpha=0.01):\n",
    "    return np.maximum(alpha * vector, vector)\n",
    "```\n",
    "\n",
    "What would be the derivative $\\displaystyle \\frac {\\partial a}{\\partial z}$? Think about how `leaky_relu` behaves differently for positive vs negative inputs.\n",
    "\n",
    "For `leaky_relu`:\n",
    "\n",
    "- if `z > 0`: $a = z$, so $\\displaystyle \\frac {\\partial a}{\\partial z} = 1$\n",
    "- if `z <= 0`: $a = \\alpha \\cdot z$, so $\\displaystyle \\frac {\\partial a}{\\partial z} = \\alpha$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e6424-7a9e-4ede-ae6c-db3986cf8fa2",
   "metadata": {},
   "source": [
    "## Neuron Class Code implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b24ef8-4031-4f6c-a1ff-15e06c5c1fa6",
   "metadata": {},
   "source": [
    "Now you have all the pieces for the gradient:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J}{\\partial \\theta_j} = (a - y_{true}) \\cdot [\\text {activation_derivative}(z)] \\cdot x_j\n",
    "$$\n",
    "\n",
    "Where $[\\text {activation_derivative}(z)]$ is $[1\\ \\text {or}\\ \\alpha]$ depends on whether $z$ was positive or negative.\n",
    "\n",
    "**Before you implement**: How would you handle computing this derivative efficiently in code, given that `z` might be a single value or a batch of values?\n",
    "\n",
    "How would you compute the gradient for all weights simultaneously, such that each weight $\\theta_j$ gets:\n",
    "\n",
    "$$\n",
    "\\text {gradient}[\\theta_j] = \\text {sum_over_examples} (\\ \\text {error} \\cdot \\text {derivative} \\cdot x_j\\ )\n",
    "$$\n",
    "\n",
    "What's your thinking about the vectorized operation needed here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2af52fee-c312-4835-9aff-ef16478da2a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:50:23.258706Z",
     "iopub.status.busy": "2025-11-09T10:50:23.258402Z",
     "iopub.status.idle": "2025-11-09T10:50:23.278528Z",
     "shell.execute_reply": "2025-11-09T10:50:23.277383Z",
     "shell.execute_reply.started": "2025-11-09T10:50:23.258680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"Implement single neuron class.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: tuple) -> None:\n",
    "        \"\"\"Initialize neuron entity.\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.w = np.random.default_rng().random(self.input_size[-1])\n",
    "        self.b = np.random.default_rng().random()\n",
    "        # activation function hyperparameter\n",
    "        self.alpha = 0.01\n",
    "\n",
    "    def leaky_relu(self, vector: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Break linearity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector : np.array\n",
    "            y_pred before activation function applied.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.maximum(self.alpha * vector, vector)\n",
    "\n",
    "    def derivative_of_leaky_relu(self, vector: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Calculate the derivative of the activation function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector : np.array\n",
    "            y_pred before activation function applied.\n",
    "\n",
    "        \"\"\"\n",
    "        derivative = np.asarray(vector, copy=True)\n",
    "        return np.where(derivative < 0, self.alpha, 1)\n",
    "\n",
    "    def _get_linear_transformation(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make the matrix multiplication of x and weights.\n",
    "\n",
    "        The result is y_pred before activation function.\n",
    "        \"\"\"\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"Calculate forward pass with activation function.\"\"\"\n",
    "        vector = self._get_linear_transformation(x)\n",
    "        return self.leaky_relu(vector)\n",
    "\n",
    "    def gradient_of_J(self, y_true: np.array, x: np.array) -> np.array:\n",
    "        \"\"\"Compute the gradient after forward pass.\"\"\"\n",
    "        y_pred = self.forward(x)\n",
    "        error = y_pred - y_true\n",
    "\n",
    "        leaky_relu_derivative = self.derivative_of_leaky_relu(\n",
    "            self._get_linear_transformation(x)\n",
    "        )\n",
    "\n",
    "        return np.dot((error * leaky_relu_derivative), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "89cde762-14af-446c-aa7a-e5cc739d5cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:51:33.543422Z",
     "iopub.status.busy": "2025-11-09T10:51:33.542927Z",
     "iopub.status.idle": "2025-11-09T10:51:33.556034Z",
     "shell.execute_reply": "2025-11-09T10:51:33.554779Z",
     "shell.execute_reply.started": "2025-11-09T10:51:33.543378Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_neuron.input_size: (4, 5)\n",
      "my_neuron.w: [0.92542368 0.63684431 0.63029109 0.48955112 0.96805626]\n",
      "linear transformation: [25.99848804 29.51617692 32.5746651  -5.27553477]\n",
      "my_neuron.forward(X): [25.99848804 29.51617692 32.5746651  -0.05275535]\n",
      "my_neuron.gradient_of_J(y_true=y, x=X): [-2439.9058491  -1478.99239077  1113.91420904 -7424.18164295\n",
      " -9327.80533079]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [-1, 2, 3, 13, 17],\n",
    "        [4, -5, 6, 14, 18],\n",
    "        [7, 8, -9, 15, 19],\n",
    "        [-10, 11, 12, 16, -20],\n",
    "    ],\n",
    "    dtype=np.float64,\n",
    ")\n",
    "y = np.array([100, 200, 300, 400], dtype=np.float64)\n",
    "\n",
    "my_neuron = Neuron((X.shape))\n",
    "print(\"my_neuron.input_size:\", my_neuron.input_size)\n",
    "print(\"my_neuron.w:\", my_neuron.w)\n",
    "print(\"linear transformation:\", my_neuron._get_linear_transformation(X))\n",
    "print(\"my_neuron.forward(X):\", my_neuron.forward(X))\n",
    "print(\n",
    "    \"my_neuron.gradient_of_J(y_true=y, x=X):\",\n",
    "    my_neuron.gradient_of_J(y_true=y, x=X),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbb10e-8a2f-4367-9025-8dd7d48afdde",
   "metadata": {},
   "source": [
    "I've carefully reviewed your code and this is a **massive improvement**. You've solved the key issues beautifully.\n",
    "\n",
    "**What You Fixed Perfectly ‚úÖ**\n",
    "\n",
    "1. **Correct derivative application**: Using `_get_linear_transformation(x)` as input to the derivative\n",
    "2. **Proper vectorization**: `error * leaky_relu_derivative` element-wise multiplication\n",
    "3. **Efficient gradient computation**: `np.dot((error * leaky_relu_derivative), x)` is exactly right!\n",
    "\n",
    "**Your Solution is Mathematically Correct**\n",
    "\n",
    "Let's trace the dimensions:\n",
    "- `error`: shape `(4,)` \n",
    "- `leaky_relu_derivative`: shape `(4,)`\n",
    "- `error * leaky_relu_derivative`: shape `(4,)` (element-wise)\n",
    "- `x`: shape `(4, 5)`\n",
    "- `np.dot(..., x)`: shape `(5,)` - perfect! One gradient per weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec6b9c-bebe-47fa-b9aa-cfaf98036c86",
   "metadata": {},
   "source": [
    "### Missing Bias Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e4f38-3e44-4549-80f4-d9324d88e607",
   "metadata": {},
   "source": [
    "**Problem**: Your `gradient_of_J` returns only `dJ_dùúÉ` but completely omits `dJ_db`!\n",
    "\n",
    "**Mathematical Reason**: \n",
    "- For bias: $\\displaystyle \\frac{\\partial z}{\\partial b} = 1$ where $z = \\theta \\cdot x + b$ (and it is always true because we always have $b^1 => 1$)\n",
    "- Therefore: $\\displaystyle \\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b}= \\text{error} \\times \\text{activation_derivative} \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0426242-3947-4b7f-8e9c-defc38cccd3a",
   "metadata": {},
   "source": [
    "**Visualizing the Bias Gradient**\n",
    "\n",
    "Think of our neuron processing a batch of 4 examples:\n",
    "\n",
    "```\n",
    "Example 1: x‚ÇÅ ‚Üí z‚ÇÅ = ùúÉ¬∑x‚ÇÅ + b ‚Üí a‚ÇÅ = leaky_relu(z‚ÇÅ) ‚Üí J = (y_true - a‚ÇÅ)¬≤\n",
    "Example 2: x‚ÇÇ ‚Üí z‚ÇÇ = ùúÉ¬∑x‚ÇÇ + b ‚Üí a‚ÇÇ = leaky_relu(z‚ÇÇ) ‚Üí J = (y_true - a‚ÇÇ)¬≤\n",
    "Example 3: x‚ÇÉ ‚Üí z‚ÇÉ = ùúÉ¬∑x‚ÇÉ + b ‚Üí a‚ÇÉ = leaky_relu(z‚ÇÉ) ‚Üí J = (y_true - a‚ÇÉ)¬≤\n",
    "Example 4: x‚ÇÑ ‚Üí z‚ÇÑ = ùúÉ¬∑x‚ÇÑ + b ‚Üí a‚ÇÑ = leaky_relu(z‚ÇÑ) ‚Üí J = (y_true - a‚ÇÑ)¬≤\n",
    "```\n",
    "\n",
    "**Key Insight**: The same bias `b` affects **every example** in the batch!\n",
    "\n",
    "**Mathematical Derivation**\n",
    "\n",
    "For squared error loss: $$J = \\frac{1}{2} \\sum_{i=1}^{m} (y_{pred}^{(i)} - y_{true}^{(i)})^2$$\n",
    "\n",
    "Let's trace the gradient flow for bias:\n",
    "\n",
    "1. **For one example**: $\\displaystyle \\frac{\\partial J^{(i)}}{\\partial b} = (y_{pred}^{(i)} - y_{true}^{(i)}) \\cdot \\text{leaky_relu}'(z^{(i)}) \\cdot 1$\n",
    "\n",
    "2. **For the whole batch**: $\\displaystyle \\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{m} \\frac{\\partial J^{(i)}}{\\partial b}$\n",
    "\n",
    "So: `dJ_db = sum_over_all_examples(error * activation_derivative)`\n",
    "\n",
    "**Why Sum Instead of Average?**\n",
    "\n",
    "Let's see what happens with a concrete batch:\n",
    "\n",
    "```python\n",
    "# Example batch of 4 examples\n",
    "errors = [0.5, -0.2, 0.3, -0.1]           # y_pred - y_true\n",
    "derivatives = [1.0, 0.01, 1.0, 0.01]      # leaky_relu_derivative\n",
    "\n",
    "# Correct: SUM (total effect of bias on all examples)\n",
    "dJ_db = (0.5*1.0) + (-0.2*0.01) + (0.3*1.0) + (-0.1*0.01) = 0.5 - 0.002 + 0.3 - 0.001 = 0.797\n",
    "\n",
    "# Wrong: AVERAGE (would underestimate total bias contribution)\n",
    "dJ_db_avg = 0.797 / 4 = 0.199  # This would make bias updates 4x too small!\n",
    "```\n",
    "\n",
    "**The intuition**: Bias is a **shared parameter** across all examples, so we need to accumulate its effect from the entire batch.\n",
    "\n",
    "**Now Let's Compare with Weight Gradients**\n",
    "\n",
    "For weights, each weight connects to specific input features:\n",
    "\n",
    "```\n",
    "Example 1: w¬∑x‚ÇÅ = w‚ÇÅ¬∑x‚ÇÅ‚ÇÅ + w‚ÇÇ¬∑x‚ÇÅ‚ÇÇ + ... + w‚Çô¬∑x‚ÇÅ‚Çô  \n",
    "Example 2: w¬∑x‚ÇÇ = w‚ÇÅ¬∑x‚ÇÇ‚ÇÅ + w‚ÇÇ¬∑x‚ÇÇ‚ÇÇ + ... + w‚Çô¬∑x‚ÇÇ‚Çô\n",
    "```\n",
    "\n",
    "The gradient for weight w‚±º comes from: $\\displaystyle \\frac{\\partial J}{\\partial w_j} = \\sum_{i=1}^{m} (\\text{error}^{(i)} \\cdot \\text{derivative}^{(i)} \\cdot x_{ij})$\n",
    "\n",
    "This is exactly what `np.dot(error * derivative, x)` computes!\n",
    "\n",
    "**Let's Build This Understanding Together**\n",
    "\n",
    "**Step 1**: Does the \"bias affects all examples equally\" intuition make sense?\n",
    "\n",
    "**Step 2**: Can you see why we sum across examples for bias gradient but use dot product for weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1475376-416e-46bc-9c75-3e5365a66510",
   "metadata": {},
   "source": [
    "### Why Sum for Bias vs Dot Product for Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0ce5a-e557-47d2-bc09-e5bb1b60738d",
   "metadata": {},
   "source": [
    "**Bias Gradient: Simple Sum**\n",
    "\n",
    "For bias, every example contributes equally:\n",
    "```\n",
    "dJ_db = sum_over_all_examples(error * activation_derivative)\n",
    "```\n",
    "\n",
    "Why? Because bias affects **every example identically** - it's like a universal offset.\n",
    "\n",
    "**Weight Gradient: Dot Product**\n",
    "\n",
    "For weights, each example contributes **differently** based on its input features:\n",
    "```\n",
    "dJ_dw = dot_product(error * activation_derivative, x)\n",
    "```\n",
    "\n",
    "Why dot product? Let's break it down:\n",
    "\n",
    "**Concrete Example**\n",
    "\n",
    "Say we have 3 examples and 2 features:\n",
    "\n",
    "```python\n",
    "# Input data (3 examples, 2 features)\n",
    "x = [[0.1, 0.2],    # example 1\n",
    "     [0.3, 0.4],    # example 2  \n",
    "     [0.5, 0.6]]    # example 3\n",
    "\n",
    "# For each example:\n",
    "errors = [0.5, -0.2, 0.3]                    # (y_pred - y_true)\n",
    "derivatives = [1.0, 1.0, 1.0]                # activation derivatives\n",
    "\n",
    "# Bias gradient (simple sum):\n",
    "dJ_db = (0.5*1.0) + (-0.2*1.0) + (0.3*1.0) = 0.5 - 0.2 + 0.3 = 0.6\n",
    "\n",
    "# Weight gradient (dot product):\n",
    "# We need: sum over examples of (error * derivative * x_ij) for each feature j\n",
    "\n",
    "# For weight w‚ÇÅ (first feature):\n",
    "dJ_dw1 = (0.5*1.0*0.1) + (-0.2*1.0*0.3) + (0.3*1.0*0.5) \n",
    "       = 0.05 + (-0.06) + 0.15 = 0.14\n",
    "\n",
    "# For weight w‚ÇÇ (second feature):  \n",
    "dJ_dw2 = (0.5*1.0*0.2) + (-0.2*1.0*0.4) + (0.3*1.0*0.6)\n",
    "       = 0.10 + (-0.08) + 0.18 = 0.20\n",
    "\n",
    "dJ_dw = [0.14, 0.20]  # This is exactly what np.dot(error * derivative, x) computes!\n",
    "```\n",
    "\n",
    "**The Key Difference**\n",
    "\n",
    "- **Bias**: Same \"contribution weight\" for every example (always √ó1)\n",
    "- **Weights**: Contribution depends on the input feature values\n",
    "\n",
    "So we accumulate across examples for both, but **how** we accumulate differs:\n",
    "\n",
    "- Bias: Simple sum (all examples treated equally)\n",
    "- Weights: Weighted sum (examples weighted by their input features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2007553-53e5-473e-946c-2a4f90dd87a0",
   "metadata": {},
   "source": [
    "### `self.forward` dependency problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b02db-0e17-4ed9-b9d7-23912602fa58",
   "metadata": {},
   "source": [
    "```python\n",
    "def gradient(self, X: np.array, y_true: np.array) -> np.array:\n",
    "    \"\"\"Gradients for squared error loss: J = (y_pred - y_true)^2.\"\"\"\n",
    "    error = self.a - y_true  # ‚Üê RELIES ON PREVIOUS forward() CALL!\n",
    "```\n",
    "\n",
    "**Problem**: `gradient()` depends on `self.a` being populated by a previous `forward()` call. This creates hidden state dependencies.\n",
    "\n",
    "**Risk**: If someone calls `gradient()` without calling `forward()` first, they get stale or missing `self.a` values.\n",
    "\n",
    "**Fix Options**:\n",
    "- Option A: Call `forward()` internally within `gradient()`\n",
    "- Option B: Require `y_pred` as parameter to `gradient()`\n",
    "- Option C: Document the dependency clearly and add validation\n",
    "\n",
    "How do you want to handle the state dependency between `forward()` and `gradient()`? This is the main architectural decision remaining.\n",
    "\n",
    "Answer: \"I decided to add self.forward to the self.gradient method. I do not like such approaches because one can call self.forward twice, but it looks like this is the standard engineering way so people know this works like this and if I do it in my style they may miss the documentation and be confused.\"\n",
    "\n",
    "You've chosen the pragmatic, standard approach.\n",
    "\n",
    "**Why This is the Right Choice**\n",
    "\n",
    "1. **Principle of Least Surprise**: Most ML practitioners expect `gradient()` to be self-contained\n",
    "2. **API Safety**: Prevents subtle bugs from incorrect call sequences  \n",
    "3. **Standard Pattern**: Matches how frameworks like PyTorch and TensorFlow work internally\n",
    "4. **Documentation Simplicity**: Reduces the cognitive load on users\n",
    "\n",
    "**The Engineering Trade-off**\n",
    "\n",
    "You correctly identified the trade-off:\n",
    "- **Your preferred style**: Explicit dependencies, cleaner separation of concerns\n",
    "- **Standard approach**: Implicit forward call, more robust for general use\n",
    "\n",
    "In systems architecture, we often choose the **more robust** pattern over the **theoretically pure** one, especially for user-facing APIs.\n",
    "\n",
    "**Performance Note**\n",
    "\n",
    "Yes, this means `forward()` might be called twice if someone does:\n",
    "```python\n",
    "y = neuron.forward(X)\n",
    "grads = neuron.gradient(X, y_true)  # forward() called again internally\n",
    "```\n",
    "\n",
    "But in practice:\n",
    "- The computational cost is negligible for a single neuron\n",
    "- The safety benefit outweighs the minor redundancy\n",
    "- In training loops, you typically only call `gradient()` anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42911c80-5458-426a-8f90-fb8ffa2cbdd9",
   "metadata": {},
   "source": [
    "## Progress Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b69b4-e347-48e5-891a-22fe66254891",
   "metadata": {},
   "source": [
    "You've successfully:\n",
    "- ‚úÖ Implemented a single neuron with Leaky ReLU activation\n",
    "- ‚úÖ Derived and implemented backpropagation gradients\n",
    "- ‚úÖ Created a working gradient computation for batch inputs\n",
    "\n",
    "**Major Conceptual Milestone Achieved**\n",
    "\n",
    "You've now bridged the gap from theoretical understanding to practical implementation of neural network fundamentals. Your gradient computation is mathematically sound and efficiently vectorized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec76bce-dce3-41ef-849b-e1f8f95b537e",
   "metadata": {},
   "source": [
    "## Ready for the Next Step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b5865-50e4-4e4a-8ccf-67902d32245d",
   "metadata": {},
   "source": [
    "Your gradient computation is working! The natural progression is:\n",
    "1. **Weight update**: Use these gradients to actually train the neuron\n",
    "2. **Training loop**: Implement gradient descent\n",
    "3. **Test on real data**: See if it can learn a simple function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8846046-0d51-4893-a9a3-3664ae4d9693",
   "metadata": {},
   "source": [
    "# <b>F.2</b> Weight update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a3b44-825f-4fb0-b273-955ab20c916e",
   "metadata": {},
   "source": [
    "# 2.1 TDD approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27081ca9-ace7-47c1-84f2-88c5fab781e5",
   "metadata": {},
   "source": [
    "## Update weights test oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895bf839-6349-41e7-a755-0e0d36ce8580",
   "metadata": {},
   "source": [
    "A **test oracle** - a separate, verified implementation used to validate your main code.\n",
    "\n",
    "**Test Oracle Pattern**: Creating an independent implementation to verify the system under test. This is valid when:\n",
    "- The oracle is simpler/more transparent than the main implementation\n",
    "- It's manually verified by a human before use\n",
    "- It serves as a reference implementation\n",
    "\n",
    "**Your framework below is actually sophisticated**:\n",
    "- It documents the complete mathematical reasoning\n",
    "- Allows manual verification of each test case\n",
    "- Creates reproducible, mathematically consistent scenarios\n",
    "- Serves as both test data AND documentation\n",
    "\n",
    "**When This Approach Is Valuable**\n",
    "\n",
    "1. **Complex mathematical systems** (like neural networks)\n",
    "2. **Learning contexts** where understanding the derivation matters\n",
    "3. **Reference implementations** for algorithm validation\n",
    "4. **Documentation of expected behavior**\n",
    "\n",
    "**The Professional Balance**\n",
    "\n",
    "In production, you'd likely:\n",
    "- Keep your detailed oracle for complex cases\n",
    "- Use simple manual tests for basic functionality\n",
    "- Have both verification strategies\n",
    "\n",
    "**Your approach is architecturally sound for a learning context**. The key is ensuring you manually verify the oracle outputs before using them to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b78b051d-a79b-4b97-b036-fed4c7393e7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T20:58:08.528121Z",
     "iopub.status.busy": "2025-11-11T20:58:08.527840Z",
     "iopub.status.idle": "2025-11-11T20:58:08.537570Z",
     "shell.execute_reply": "2025-11-11T20:58:08.536143Z",
     "shell.execute_reply.started": "2025-11-11T20:58:08.528097Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_backprop_test_case(case_name, X, y_true, w, b, lr, activation_f=\"leaky_relu\", cost_f=\"mse\",):\n",
    "    \"\"\"Create backpropagation test case.\"\"\"\n",
    "    # final result\n",
    "    result = {\n",
    "        \"name\": case_name,\n",
    "        \"initial_X\": X,\n",
    "        \"y_true\": y_true,\n",
    "        \"initial_w\": w,\n",
    "        \"initial_b\": b,\n",
    "        \"learning_rate\": lr\n",
    "    }\n",
    "\n",
    "    # linear transformation\n",
    "    z = np.dot(X, w) + b\n",
    "    result[\"z\"] = z\n",
    "    \n",
    "    # activation function\n",
    "    alpha = 0.01\n",
    "    if activation_f == \"leaky_relu\":\n",
    "        a = np.maximum(z * alpha, z)\n",
    "    result[\"a\"] = a\n",
    "\n",
    "    # cost function\n",
    "    if cost_f == \"mse\":\n",
    "        u = a - y_true\n",
    "        J = u**2 / 2\n",
    "    else:\n",
    "        raise ValueError(f\"Cost function \\\"{cost_function}\\\" is not implemented yet.\")\n",
    "\n",
    "    result[\"J\"] = J\n",
    "\n",
    "    # backpropagation\n",
    "    \n",
    "    # J'\n",
    "    dJ_da = u\n",
    "    result[\"dJ_da\"] = dJ_da\n",
    "    \n",
    "    # a'\n",
    "    da_dz = np.where(z < 0, alpha, 1)\n",
    "    result[\"da_dz\"] = da_dz\n",
    "    \n",
    "    # z'\n",
    "    dz_dw = X\n",
    "    result[\"dz_dw\"] = dz_dw\n",
    "\n",
    "    # b'\n",
    "    dz_b = 1\n",
    "    result[\"dz_b\"] = dz_b\n",
    "\n",
    "    dJ_dw = np.dot(dJ_da * da_dz, dz_dw)  # where dz_dw = X\n",
    "    result[\"dJ_dw\"] = dJ_dw\n",
    "\n",
    "    dJ_db = np.sum(dJ_da * da_dz)*dz_b  # where dz_b = 1\n",
    "    result[\"dJ_db\"] = dJ_db\n",
    "\n",
    "    # w - lr * (-gradient) = w + lr*|gradient|\n",
    "    result[\"expected_w\"] = np.subtract(w, (lr * dJ_dw))\n",
    "    result[\"expected_b\"] = np.subtract(b, (lr * dJ_db))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff1954cf-30e7-41a9-9167-fc7ce743f73d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T20:58:09.032884Z",
     "iopub.status.busy": "2025-11-11T20:58:09.032071Z",
     "iopub.status.idle": "2025-11-11T20:58:09.063645Z",
     "shell.execute_reply": "2025-11-11T20:58:09.060700Z",
     "shell.execute_reply.started": "2025-11-11T20:58:09.032816Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'negative_gradients_increase_weights',\n",
       " 'initial_X': array([[ 1., -2.]]),\n",
       " 'y_true': 1.5,\n",
       " 'initial_w': array([0.3, 0.4]),\n",
       " 'initial_b': 0.1,\n",
       " 'learning_rate': 0.01,\n",
       " 'z': array([-0.4]),\n",
       " 'a': array([-0.004]),\n",
       " 'J': array([1.131008]),\n",
       " 'dJ_da': array([-1.504]),\n",
       " 'da_dz': array([0.01]),\n",
       " 'dz_dw': array([[ 1., -2.]]),\n",
       " 'dz_b': 1,\n",
       " 'dJ_dw': array([-0.01504,  0.03008]),\n",
       " 'dJ_db': np.float64(-0.01504),\n",
       " 'expected_w': array([0.3001504, 0.3996992]),\n",
       " 'expected_b': np.float64(0.1001504)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case1 = create_backprop_test_case(\n",
    "    \"negative_gradients_increase_weights\",\n",
    "     np.array([[1.0, -2.0]]),\n",
    "     y_true = 1.5,\n",
    "     w = np.array([0.3, 0.4]),\n",
    "     b = 0.1,\n",
    "     lr = 0.01\n",
    ")\n",
    "case1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb622de-db04-4ab0-beee-952ad64da28c",
   "metadata": {},
   "source": [
    "Your gradient computation is now mathematically correct.\n",
    "\n",
    "These gradients make perfect sense:\n",
    "- The weight gradients are proportional to their input values (1.0 and 2.0)\n",
    "- All gradients point downward (negative) since we need to increase the weights to reduce the loss\n",
    "\n",
    "We now have mathematically consistent test data:\n",
    "- Initial: `w = [0.3, 0.4]`, `b = 0.1`\n",
    "- Gradients: `dJ_dw = [-0.6, -1.2]`, `dJ_db = [-0.6]`\n",
    "- Learning rate: `0.01`\n",
    "\n",
    "**Gradient Descent Update:**\n",
    "```\n",
    "w_new = w_old - learning_rate * gradient\n",
    "```\n",
    "\n",
    "If gradient is **negative**, then:\n",
    "```\n",
    "w_new = w_old - learning_rate * (negative_number)\n",
    "w_new = w_old + learning_rate * positive_number\n",
    "```\n",
    "\n",
    "**In our case:**\n",
    "- Gradients are all negative: `[-0.3, -0.6]` and `[-0.3]`\n",
    "- This means we need to increase weights and bias to reduce loss\n",
    "\n",
    "**Expected after update:**\n",
    "```\n",
    "w_new = [0.3, 0.4] - 0.01 * [-0.3, -0.6] = [0.303, 0.406]\n",
    "b_new = 0.1 - 0.01 * [-0.3] = [0.103]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea5df84b-d8be-4e31-bc4a-a5190549ee1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T16:16:12.598845Z",
     "iopub.status.busy": "2025-11-11T16:16:12.598484Z",
     "iopub.status.idle": "2025-11-11T16:16:12.609359Z",
     "shell.execute_reply": "2025-11-11T16:16:12.608244Z",
     "shell.execute_reply.started": "2025-11-11T16:16:12.598818Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'negative_gradients_increase_weights',\n",
       " 'initial_X': array([[1., 2.]]),\n",
       " 'y_true': 1.5,\n",
       " 'initial_w': array([0.3, 0.4]),\n",
       " 'initial_b': 0.1,\n",
       " 'learning_rate': 0.01,\n",
       " 'z': array([1.2]),\n",
       " 'J': array([0.045]),\n",
       " 'dJ_dz': array([-0.3]),\n",
       " 'dz_b': 1,\n",
       " 'dJ_dw': array([-0.3, -0.6]),\n",
       " 'dJ_db': array([-0.3]),\n",
       " 'expected_w': array([0.303, 0.406]),\n",
       " 'expected_b': array([0.103])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16889a2f-89c3-401e-a5fd-42faafaf1178",
   "metadata": {},
   "source": [
    "Create more test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d88e9-ac7f-4727-8b60-4afade7a64f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T16:16:49.972333Z",
     "iopub.status.busy": "2025-11-11T16:16:49.971848Z",
     "iopub.status.idle": "2025-11-11T16:16:50.004409Z",
     "shell.execute_reply": "2025-11-11T16:16:50.003144Z",
     "shell.execute_reply.started": "2025-11-11T16:16:49.972287Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'negative_gradients_increase_weights',\n",
       "  'initial_X': array([[1., 2.]]),\n",
       "  'y_true': 1.5,\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'z': array([1.2]),\n",
       "  'J': array([0.045]),\n",
       "  'dJ_dz': array([-0.3]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-0.3, -0.6]),\n",
       "  'dJ_db': array([-0.3]),\n",
       "  'expected_w': array([0.303, 0.406]),\n",
       "  'expected_b': array([0.103])},\n",
       " {'name': 'zero_gradients_no_change',\n",
       "  'initial_X': array([[1., 2.]]),\n",
       "  'y_true': 1.2,\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'z': array([1.2]),\n",
       "  'J': array([2.46519033e-32]),\n",
       "  'dJ_dz': array([2.22044605e-16]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([2.22044605e-16, 4.44089210e-16]),\n",
       "  'dJ_db': array([2.22044605e-16]),\n",
       "  'expected_w': array([0.3, 0.4]),\n",
       "  'expected_b': array([0.1])},\n",
       " {'name': 'large_learning_rate',\n",
       "  'initial_X': array([[1., 2.]]),\n",
       "  'y_true': 2.0,\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.1,\n",
       "  'z': array([1.2]),\n",
       "  'J': array([0.32]),\n",
       "  'dJ_dz': array([-0.8]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-0.8, -1.6]),\n",
       "  'dJ_db': array([-0.8]),\n",
       "  'expected_w': array([0.38, 0.56]),\n",
       "  'expected_b': array([0.18])},\n",
       " {'name': 'batch_input_gradients',\n",
       "  'initial_X': array([[1. , 2. ],\n",
       "         [0.5, 1. ]]),\n",
       "  'y_true': array([1.5, 0.8]),\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'z': array([1.2 , 0.65]),\n",
       "  'J': array([0.045  , 0.01125]),\n",
       "  'dJ_dz': array([-0.3 , -0.15]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-0.375, -0.75 ]),\n",
       "  'dJ_db': array([-0.3 , -0.15]),\n",
       "  'expected_w': array([0.30375, 0.4075 ]),\n",
       "  'expected_b': array([0.103 , 0.1015])},\n",
       " {'name': 'single_feature',\n",
       "  'initial_X': array([[1.]]),\n",
       "  'y_true': 0.5,\n",
       "  'initial_w': array([0.3]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'z': array([0.4]),\n",
       "  'J': array([0.005]),\n",
       "  'dJ_dz': array([-0.1]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-0.1]),\n",
       "  'dJ_db': array([-0.1]),\n",
       "  'expected_w': array([0.301]),\n",
       "  'expected_b': array([0.101])},\n",
       " {'name': 'extreme_gradients',\n",
       "  'initial_X': array([[100., 200.]]),\n",
       "  'y_true': 500.0,\n",
       "  'initial_w': array([0.3, 0.4]),\n",
       "  'initial_b': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'z': array([110.1]),\n",
       "  'J': array([76011.005]),\n",
       "  'dJ_dz': array([-389.9]),\n",
       "  'dz_b': 1,\n",
       "  'dJ_dw': array([-38990., -77980.]),\n",
       "  'dJ_db': array([-389.9]),\n",
       "  'expected_w': array([39.29, 78.38]),\n",
       "  'expected_b': array([0.4899])}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edge case: Zero gradients (no learning should occur)\n",
    "case2 = create_backprop_test_case(\n",
    "    \"zero_gradients_no_change\",\n",
    "    np.array([[1.0, 2.0]]),\n",
    "    y_true=1.2,  # Exactly matches our forward pass output\n",
    "    w=np.array([0.3, 0.4]),\n",
    "    b=0.1,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Edge case: Large learning rate\n",
    "case3 = create_backprop_test_case(\n",
    "    \"large_learning_rate\",\n",
    "    np.array([[1.0, 2.0]]),\n",
    "    y_true=2.0,  # Larger error\n",
    "    w=np.array([0.3, 0.4]),\n",
    "    b=0.1,\n",
    "    lr=0.1  # 10x larger learning rate\n",
    ")\n",
    "\n",
    "# Batch input case (multiple examples)\n",
    "case4 = create_backprop_test_case(\n",
    "    \"batch_input_gradients\",\n",
    "    np.array([[1.0, 2.0], [0.5, 1.0]]),  # 2 examples\n",
    "    y_true=np.array([1.5, 0.8]),  # Batch targets\n",
    "    w=np.array([0.3, 0.4]),\n",
    "    b=0.1,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Single feature case\n",
    "case5 = create_backprop_test_case(\n",
    "    \"single_feature\",\n",
    "    np.array([[1.0]]),  # Single input feature\n",
    "    y_true=0.5,\n",
    "    w=np.array([0.3]),\n",
    "    b=0.1,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Extreme values case\n",
    "case6 = create_backprop_test_case(\n",
    "    \"extreme_gradients\",\n",
    "    np.array([[100.0, 200.0]]),  # Large inputs\n",
    "    y_true=500.0,  # Large target\n",
    "    w=np.array([0.3, 0.4]),\n",
    "    b=0.1,\n",
    "    lr=0.001  # Smaller LR for stability\n",
    ")\n",
    "\n",
    "test_cases = [case1, case2, case3, case4, case5, case6]\n",
    "\n",
    "test_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395a1be-4d80-41ac-90ff-c046972309f5",
   "metadata": {},
   "source": [
    "## TDD cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552773e-08ed-4d24-bf46-105f134358f8",
   "metadata": {},
   "source": [
    "We follow the TDD cycle: **Red** ‚Üí **Green** ‚Üí **Refactor**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e16c3-aa81-473f-84ca-38b0189e3fc9",
   "metadata": {},
   "source": [
    "### Current Phase: RED (Write Failing Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fbc7a-97f9-4af9-86f9-7d126eb3660c",
   "metadata": {},
   "source": [
    "Using your test oracle data:\n",
    "\n",
    "```python\n",
    "def test_weight_update_negative_gradients():\n",
    "    \"\"\"Test that negative gradients correctly increase weights.\"\"\"\n",
    "    neuron = Neuron()\n",
    "    \n",
    "    # Setup initial state from oracle\n",
    "    neuron.w = np.array([0.3, 0.4])\n",
    "    neuron.b = 0.1\n",
    "    \n",
    "    # Gradients from oracle calculation\n",
    "    dJ_dw = np.array([-0.3, -0.6])\n",
    "    dJ_db = -0.3\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Call method (doesn't exist yet - this will fail)\n",
    "    neuron.update_weights(dJ_dw, dJ_db, learning_rate)\n",
    "    \n",
    "    # Expected results from oracle\n",
    "    expected_w = np.array([0.303, 0.406])\n",
    "    expected_b = 0.103\n",
    "    \n",
    "    # Assertions (will fail initially)\n",
    "    np.testing.assert_array_almost_equal(neuron.w, expected_w, decimal=6)\n",
    "    assert abs(neuron.b - expected_b) < 1e-6\n",
    "```\n",
    "\n",
    "**Your turn**: This test will fail because `update_weights` doesn't exist yet. Should we:\n",
    "1. Write this test and watch it fail (Red phase)\n",
    "2. Then implement the minimal `update_weights` method to make it pass (Green phase)\n",
    "\n",
    "*(I'll wait for your confirmation to proceed with the test)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e10b2-5679-4b0c-8328-3e2b22996868",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb45c5bf-2616-4de5-935f-e2cb97827b1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b430f080-1a8e-45ad-8ab6-9c74d3569ab8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e237b871-7a0d-4e8d-a5db-b15b74b4d22f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be4b250d-8300-472b-933d-bed4b0c26cca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1cfb6d7-2ea2-4bbc-b199-f73bb589c253",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4446bb18-4532-4fce-9108-82e049bfa896",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9c49f37-db7c-484c-a223-b67d54ff9467",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_learning",
   "language": "python",
   "name": "pytorch_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa725d5-2957-4536-8ec8-82ed546d6b03",
   "metadata": {},
   "source": [
    "# FOUNDATION F: NEURAL NETWORKS FROM SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b605a9-7d78-4433-888b-8d2da4e22624",
   "metadata": {},
   "source": [
    "# <b>F.1 Gradient</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca004a6-7e46-40ac-b5a6-761a330dfa83",
   "metadata": {},
   "source": [
    "**The Core Idea**: A neural network is just a mathematical function that can be represented as a computational graph. The \"learning\" happens by adjusting the parameters of this function to minimize some error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b34ae0-059b-4d9e-9f08-a5e446df7738",
   "metadata": {},
   "source": [
    "Think about a single neuron - the simplest building block. If you were to implement this from scratch in NumPy (no PyTorch yet), what would be the minimal components you'd need?\n",
    "\n",
    "Consider:\n",
    "- What inputs does it take?\n",
    "- What parameters does it have? \n",
    "- What computation does it perform?\n",
    "- What output does it produce?\n",
    "\n",
    "1. **Linear combination**: $y = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b$\n",
    "2. **Weights initialization**: Random values\n",
    "3. **Activation function**: Breaks linearity\n",
    "4. **Matrix operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3792ba-fda5-4d5d-bc6b-bab4ae81910f",
   "metadata": {},
   "source": [
    "## The Linear Algebra Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7f8ef-17c0-4c30-abe3-5a31ec3f4489",
   "metadata": {},
   "source": [
    "Think about processing one input vector vs many inputs:\n",
    "\n",
    "**Single input**: $[x_1, x_2, x_3]$ with weights $[\\theta_1, \\theta_2, \\theta_3]$  \n",
    "$output = x_1\\theta_1 + x_2\\theta_2 + x_3\\theta_3 + b$\n",
    "\n",
    "**Multiple inputs (as matrix)**: \n",
    "```\n",
    "Inputs: [ [xâ‚â‚, xâ‚â‚‚, xâ‚â‚ƒ],   Weights: [ðœƒâ‚, ðœƒâ‚‚, ðœƒâ‚ƒ]áµ€\n",
    "          [xâ‚‚â‚, xâ‚‚â‚‚, xâ‚‚â‚ƒ],\n",
    "          [xâ‚ƒâ‚, xâ‚ƒâ‚‚, xâ‚ƒâ‚ƒ] ]\n",
    "```\n",
    "\n",
    "What linear algebra operation would efficiently compute all outputs at once? Matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b372b9-f12c-40b2-b2ec-8a53e3f8cfd5",
   "metadata": {},
   "source": [
    "## Single neuron code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28bba8-a3a1-4c24-a25b-04a971ea77dd",
   "metadata": {},
   "source": [
    "Try implementing the **single input case** first:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize weights and bias here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute y = wÂ·x + b\n",
    "        # Then apply activation function\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772379a-5a41-4cdc-856b-6c0e9e7dfd6d",
   "metadata": {},
   "source": [
    "## Activation Function Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2b2f0-d6ad-45c5-a8be-8861abf56c4b",
   "metadata": {},
   "source": [
    "| Function | Formula | Range | Key Properties |\n",
    "|----------|---------|-------|----------------|\n",
    "| **Sigmoid** | 1/(1+eâ»Ë£) | (0,1) | Smooth, bounded, but can saturate (vanishing gradients) |\n",
    "| **Tanh** | (eË£-eâ»Ë£)/(eË£+eâ»Ë£) | (-1,1) | Zero-centered, but still can saturate |\n",
    "| **ReLU** | max(0,x) | [0,âˆž) | Simple, avoids saturation, but \"dying ReLU\" problem |\n",
    "| **Leaky ReLU** | max(0.01x,x) | (-âˆž,âˆž) | Fixes dying ReLU, small gradient for negatives |\n",
    "\n",
    "**Historical Context & Modern Practice**\n",
    "\n",
    "- **1980s-2000s**: Sigmoid/tanh were dominant (biological plausibility)\n",
    "- **2010s**: ReLU became standard for hidden layers (training speed)\n",
    "- **Today**: Variants like Leaky ReLU, GELU are common\n",
    "\n",
    "For your **first from-scratch implementation**, which activation function would you choose and **why**? Consider:\n",
    "\n",
    "1. **Simplicity of implementation**\n",
    "2. **Gradient behavior** (important for backpropagation)\n",
    "3. **Training stability**\n",
    "\n",
    "\n",
    "\n",
    "Leaky ReLU is actually a very smart pick for your first implementation.\n",
    "\n",
    "**Why Leaky ReLU is a Great Choice**\n",
    "\n",
    "1. **Simple to implement** - just `max(0.01*x, x)` \n",
    "2. **Avoids the \"dying neuron\" problem** of regular ReLU\n",
    "3. **Computationally efficient** - no expensive exponentials like sigmoid/tanh\n",
    "4. **Good gradient flow** - never completely zero gradient\n",
    "\n",
    "**Your Implementation Challenge**\n",
    "\n",
    "Now, here's what I want you to think about for the implementation:\n",
    "\n",
    "```python\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    # Your implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Key consideration**: Should this be a method of your `Neuron` class, or a separate function? What are the trade-offs?\n",
    "\n",
    "Also think about: How will this affect your backpropagation implementation later? (We'll get to that soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423ddfb-c9b4-4254-aa5e-c9c52f46d4c9",
   "metadata": {},
   "source": [
    "Putting the activation function in the forward pass is exactly right.\n",
    "\n",
    "**Your Architectural Decision Makes Sense**\n",
    "\n",
    "Having the activation as part of the neuron's forward pass:\n",
    "- Keeps the neuron self-contained\n",
    "- Makes the computation clear: `output = activation(linear_combination)`\n",
    "- Will make backpropagation more straightforward (when we get there)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23676f4f-e2cb-42a1-bac6-c223ec807387",
   "metadata": {},
   "source": [
    "### Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c44b2369-d4ca-424d-a998-e8b79d9bef41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:02:08.298334Z",
     "iopub.status.busy": "2025-11-09T10:02:08.294394Z",
     "iopub.status.idle": "2025-11-09T10:02:08.428169Z",
     "shell.execute_reply": "2025-11-09T10:02:08.426235Z",
     "shell.execute_reply.started": "2025-11-09T10:02:08.298028Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"Implement single neuron class.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: tuple) -> None:\n",
    "        \"\"\"Initialize neuron entity.\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.w = np.random.default_rng().random(self.input_size[-1])\n",
    "        self.b = np.random.default_rng().random()\n",
    "        # activation function hyperparameter\n",
    "        self.alpha = 0.01\n",
    "\n",
    "    def leaky_relu(self, vector: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Break linearity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector : np.array\n",
    "            y_pred before activation function applied.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.maximum(self.alpha * vector, vector)\n",
    "\n",
    "    def _get_linear_transformation(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make the matrix multiplication of x and weights.\n",
    "\n",
    "        The result is y_pred before activation function.\n",
    "        \"\"\"\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"Calculate forward pass with activation function.\"\"\"\n",
    "        vector = self._get_linear_transformation(x)\n",
    "        return self.leaky_relu(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44f62454-e623-4b52-bedf-3e09d403e38b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:02:48.032181Z",
     "iopub.status.busy": "2025-11-09T10:02:48.031252Z",
     "iopub.status.idle": "2025-11-09T10:02:48.053346Z",
     "shell.execute_reply": "2025-11-09T10:02:48.051971Z",
     "shell.execute_reply.started": "2025-11-09T10:02:48.032041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_neuron.input_size: (4, 5)\n",
      "my_neuron.w: [0.96963449 0.88127309 0.29762979 0.32335681 0.75624471]\n",
      "linear transformation: [19.55678708 20.20853882 31.18914713 -5.57078139]\n",
      "my_neuron.forward(X): [19.55678708 20.20853882 31.18914713 -0.05570781]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [-1, 2, 3, 13, 17],\n",
    "        [4, -5, 6, 14, 18],\n",
    "        [7, 8, -9, 15, 19],\n",
    "        [-10, 11, 12, 16, -20],\n",
    "    ],\n",
    "    dtype=np.float64,\n",
    ")\n",
    "\n",
    "my_neuron = Neuron((X.shape))\n",
    "print(\"my_neuron.input_size:\", my_neuron.input_size)\n",
    "print(\"my_neuron.w:\", my_neuron.w)\n",
    "print(\"linear transformation:\", my_neuron._get_linear_transformation(X))\n",
    "print(\"my_neuron.forward(X):\", my_neuron.forward(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b044b-8c62-4be7-b29b-cb111c7ba50b",
   "metadata": {},
   "source": [
    "## Gradient derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00178c04-038b-44b4-991d-c2be8aabaccf",
   "metadata": {},
   "source": [
    "Your single neuron implementation is solid. The next critical concept is: **how does learning actually happen?**\n",
    "\n",
    "We have this neuron that can compute outputs, but how do we adjust `self.w` and `self.b` to make it produce better outputs?\n",
    "\n",
    "What's your current understanding of how the \"learning\" process works in neural networks?\n",
    "\n",
    "You've identified the three key components:\n",
    "\n",
    "1. **Error/Loss**\n",
    "2. **Gradient**\n",
    "3. **Weight Update**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68fe6d-16c5-4c20-9d18-3336c2defe09",
   "metadata": {},
   "source": [
    "### The Missing Piece: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65338f-3f5c-4ee2-b33a-e3778f2bc80c",
   "metadata": {},
   "source": [
    "What you described is **gradient descent**. The magic that makes it work for neural networks is **backpropagation** - the algorithm for efficiently computing those gradients through the entire computational graph.\n",
    "\n",
    "Think of it this way: If your neuron's computation is:\n",
    "```\n",
    "input â†’ linear â†’ activation â†’ output\n",
    "```\n",
    "\n",
    "Backpropagation answers:\n",
    "\n",
    "> \"*How much did each weight contribute to the final error?*\"\n",
    "\n",
    "Let's break this down into manageable steps. If you were to implement gradient calculation for your single neuron, what would be the minimal information you'd need?\n",
    "\n",
    "Consider:\n",
    "- The error at the output\n",
    "- How the activation function affects the gradient\n",
    "- How the linear combination affects the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339eab2-5b5b-4131-b214-d6326450b767",
   "metadata": {},
   "source": [
    "### The Error Signal Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa634e-af90-46e6-bfe4-6ee84cad1d4f",
   "metadata": {},
   "source": [
    "The error signal begins with the difference between predicted and true values.\n",
    "\n",
    "You're correct: `error = true_y - predicted_y` (or some function of this difference)\n",
    "\n",
    "Now here's the key insight for backpropagation: **We need to trace how this error flows backward through each operation.**\n",
    "\n",
    "Think about your neuron's forward pass:\n",
    "```\n",
    "x â†’ linear (wÂ·x + b) â†’ leaky_relu â†’ y_pred\n",
    "```\n",
    "\n",
    "For backpropagation, we go backward:\n",
    "```\n",
    "error â† d_leaky_relu â† d_linear â† gradients for w and b\n",
    "```\n",
    "\n",
    "If you were to implement this gradient calculation manually for a single input, what operations would you need to compute?\n",
    "\n",
    "Consider:\n",
    "1. Gradient of the error with respect to the output\n",
    "2. How the activation function transforms this gradient\n",
    "3. How the linear operation distributes gradients to weights and bias\n",
    "\n",
    "**What's your thinking about the mathematical operations involved?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ba838-8872-4371-a8ba-717b562a4232",
   "metadata": {},
   "source": [
    "### Chain Rule Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94c36f-224e-4864-a266-0f0716d11604",
   "metadata": {},
   "source": [
    "Think about your neuron's computation:\n",
    "```\n",
    "output = leaky_relu(wÂ·x + b)\n",
    "error = (true_y - output)Â²\n",
    "```\n",
    "\n",
    "If we want to know how much to change `wâ‚`, we need to answer: \"How does changing `wâ‚` affect the final error?\"\n",
    "\n",
    "This is where the **chain rule** from calculus comes in. We break the problem into steps:\n",
    "\n",
    "1. How does error change with output?\n",
    "2. How does output change with activation input?\n",
    "3. How does activation input change with `wâ‚`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef07f8b-494a-4246-8698-c2631fa79d92",
   "metadata": {},
   "source": [
    "You've done exactly the right preparation. Re-reading the math and connecting it to CS229 shows excellent learning discipline.\n",
    "\n",
    "Your understanding is spot on:\n",
    "\n",
    "- **J(Î¸)** is our cost function (like MSE)\n",
    "- **Partial derivatives** give us the gradient âˆ‡J(Î¸)  \n",
    "- **Gradient descent** uses this to update weights: Î¸ = Î¸ - Î±âˆ‡J(Î¸)\n",
    "\n",
    "You asked: *\"Do we really need to work with the error itself?\"* The answer reveals the elegance of backpropagation:\n",
    "\n",
    "> We don't actually need the explicit mathematical formula for J(Î¸). \n",
    "\n",
    "Instead, we use the **chain rule** to compute gradients through the computational graph.\n",
    "\n",
    "Think about your neuron:\n",
    "```\n",
    "x â†’ z = wÂ·x + b â†’ a = leaky_relu(z) â†’ J = (y_true - a)Â²\n",
    "```\n",
    "\n",
    "To find $\\displaystyle \\frac {\\partial}{\\partial \\theta}J(\\theta)$, we can compute:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta}J(\\theta) = \\frac {\\partial J(\\theta)}{\\partial a} \\times \\frac {\\partial a}{\\partial z} \\times \\frac {\\partial z}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "**Your implementation challenge**: If you were to compute these partial derivatives numerically for a single example, what would be your step-by-step approach?\n",
    "\n",
    "(see CS229, Batch Gradient Descent)\n",
    "\\begin{align*}\n",
    "\\frac {\\partial}{\\partial \\theta_j} J(\\theta) & = \\frac {\\partial}{\\partial \\theta_j} \\frac{1}{2} \\left(h_{\\theta}(x) - y\\right)^2 = \\\\\n",
    "& = 2 \\frac {1}{2} (h_{\\theta}(x) - y) \\cdot \\frac {\\partial}{\\partial \\theta_j}(h_{\\theta}(x) - y) = \\\\\n",
    "& = (h_{\\theta}(x) - y) \\cdot \\frac {\\partial}{\\partial \\theta_j} (\\theta_0 x_0 + \\theta_1 x_1 + \\cdots + \\theta_j x_j \\cdots + \\theta_n x_n -y) \\\\\n",
    "& = (h_{\\theta}(x) - y) \\cdot x_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51584f-0f8f-43c7-84bf-ecb9b19b0685",
   "metadata": {},
   "source": [
    "You've derived the gradient for linear regression perfectly! \n",
    "\n",
    "Your derivation:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta_j} J(\\theta) = (h_{\\theta}(x) - y) \\cdot x_j\n",
    "$$\n",
    "\n",
    "The gradient for each weight is proportional to:\n",
    "1. The **error signal** (prediction - true value)\n",
    "2. The **input** that weight connects to\n",
    "\n",
    "Your neuron has one extra step: the activation function. So the chain becomes:\n",
    "\n",
    "For weight $\\theta_j$:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial}{\\partial \\theta_j}J(\\theta) = \\frac {\\partial J(\\theta)}{\\partial a} \\times \\frac {\\partial a}{\\partial z} \\times \\frac {\\partial z}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $a = \\text {leaky_relu}(z)$ = final output\n",
    "- $z = \\theta x + b$ = linear combination\n",
    "- $\\displaystyle \\frac {\\partial z}{\\partial \\theta_j} = x_j$ (as you correctly derived)\n",
    "\n",
    "Given your `leaky_relu` function:\n",
    "```python\n",
    "def leaky_relu(self, vector, alpha=0.01):\n",
    "    return np.maximum(alpha * vector, vector)\n",
    "```\n",
    "\n",
    "What would be the derivative $\\displaystyle \\frac {\\partial a}{\\partial z}$? Think about how `leaky_relu` behaves differently for positive vs negative inputs.\n",
    "\n",
    "For `leaky_relu`:\n",
    "\n",
    "- if `z > 0`: $a = z$, so $\\displaystyle \\frac {\\partial a}{\\partial z} = 1$\n",
    "- if `z <= 0`: $a = \\alpha \\cdot z$, so $\\displaystyle \\frac {\\partial a}{\\partial z} = \\alpha$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e6424-7a9e-4ede-ae6c-db3986cf8fa2",
   "metadata": {},
   "source": [
    "### Code implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b24ef8-4031-4f6c-a1ff-15e06c5c1fa6",
   "metadata": {},
   "source": [
    "Now you have all the pieces for the gradient:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial J}{\\partial \\theta_j} = (a - y_{true}) \\cdot [\\text {derivative_of_leaky_relu}(z)] \\cdot x_j\n",
    "$$\n",
    "\n",
    "Where $[\\text {derivative_of_leaky_relu}(z)]$ is $[1\\ \\text {or}\\ \\alpha]$ depends on whether $z$ was positive or negative.\n",
    "\n",
    "**Before you implement**: How would you handle computing this derivative efficiently in code, given that `z` might be a single value or a batch of values?\n",
    "\n",
    "How would you compute the gradient for all weights simultaneously, such that each weight $\\theta_j$ gets:\n",
    "\n",
    "$$\n",
    "\\text {gradient}[\\theta_j] = \\text {sum_over_examples} (\\ \\text {error} \\cdot \\text {derivative} \\cdot x_j\\ )\n",
    "$$\n",
    "\n",
    "What's your thinking about the vectorized operation needed here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2af52fee-c312-4835-9aff-ef16478da2a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:50:23.258706Z",
     "iopub.status.busy": "2025-11-09T10:50:23.258402Z",
     "iopub.status.idle": "2025-11-09T10:50:23.278528Z",
     "shell.execute_reply": "2025-11-09T10:50:23.277383Z",
     "shell.execute_reply.started": "2025-11-09T10:50:23.258680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"Implement single neuron class.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: tuple) -> None:\n",
    "        \"\"\"Initialize neuron entity.\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.w = np.random.default_rng().random(self.input_size[-1])\n",
    "        self.b = np.random.default_rng().random()\n",
    "        # activation function hyperparameter\n",
    "        self.alpha = 0.01\n",
    "\n",
    "    def leaky_relu(self, vector: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Break linearity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector : np.array\n",
    "            y_pred before activation function applied.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.maximum(self.alpha * vector, vector)\n",
    "\n",
    "    def derivative_of_leaky_relu(self, vector: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Calculate the derivative of the activation function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector : np.array\n",
    "            y_pred before activation function applied.\n",
    "\n",
    "        \"\"\"\n",
    "        derivative = np.asarray(vector, copy=True)\n",
    "        return np.where(derivative < 0, self.alpha, 1)\n",
    "\n",
    "    def _get_linear_transformation(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make the matrix multiplication of x and weights.\n",
    "\n",
    "        The result is y_pred before activation function.\n",
    "        \"\"\"\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"Calculate forward pass with activation function.\"\"\"\n",
    "        vector = self._get_linear_transformation(x)\n",
    "        return self.leaky_relu(vector)\n",
    "\n",
    "    def gradient_of_J(self, y_true: np.array, x: np.array) -> np.array:\n",
    "        \"\"\"Compute the gradient after forward pass.\"\"\"\n",
    "        y_pred = self.forward(x)\n",
    "        error = y_pred - y_true\n",
    "\n",
    "        leaky_relu_derivative = self.derivative_of_leaky_relu(\n",
    "            self._get_linear_transformation(x)\n",
    "        )\n",
    "\n",
    "        return np.dot((error * leaky_relu_derivative), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "89cde762-14af-446c-aa7a-e5cc739d5cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:51:33.543422Z",
     "iopub.status.busy": "2025-11-09T10:51:33.542927Z",
     "iopub.status.idle": "2025-11-09T10:51:33.556034Z",
     "shell.execute_reply": "2025-11-09T10:51:33.554779Z",
     "shell.execute_reply.started": "2025-11-09T10:51:33.543378Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_neuron.input_size: (4, 5)\n",
      "my_neuron.w: [0.92542368 0.63684431 0.63029109 0.48955112 0.96805626]\n",
      "linear transformation: [25.99848804 29.51617692 32.5746651  -5.27553477]\n",
      "my_neuron.forward(X): [25.99848804 29.51617692 32.5746651  -0.05275535]\n",
      "my_neuron.gradient_of_J(y_true=y, x=X): [-2439.9058491  -1478.99239077  1113.91420904 -7424.18164295\n",
      " -9327.80533079]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [-1, 2, 3, 13, 17],\n",
    "        [4, -5, 6, 14, 18],\n",
    "        [7, 8, -9, 15, 19],\n",
    "        [-10, 11, 12, 16, -20],\n",
    "    ],\n",
    "    dtype=np.float64,\n",
    ")\n",
    "y = np.array([100, 200, 300, 400], dtype=np.float64)\n",
    "\n",
    "my_neuron = Neuron((X.shape))\n",
    "print(\"my_neuron.input_size:\", my_neuron.input_size)\n",
    "print(\"my_neuron.w:\", my_neuron.w)\n",
    "print(\"linear transformation:\", my_neuron._get_linear_transformation(X))\n",
    "print(\"my_neuron.forward(X):\", my_neuron.forward(X))\n",
    "print(\n",
    "    \"my_neuron.gradient_of_J(y_true=y, x=X):\",\n",
    "    my_neuron.gradient_of_J(y_true=y, x=X),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbb10e-8a2f-4367-9025-8dd7d48afdde",
   "metadata": {},
   "source": [
    "I've carefully reviewed your code and this is a **massive improvement**. You've solved the key issues beautifully.\n",
    "\n",
    "**What You Fixed Perfectly âœ…**\n",
    "\n",
    "1. **Correct derivative application**: Using `_get_linear_transformation(x)` as input to the derivative\n",
    "2. **Proper vectorization**: `error * leaky_relu_derivative` element-wise multiplication\n",
    "3. **Efficient gradient computation**: `np.dot((error * leaky_relu_derivative), x)` is exactly right!\n",
    "\n",
    "**Your Solution is Mathematically Correct**\n",
    "\n",
    "Let's trace the dimensions:\n",
    "- `error`: shape `(4,)` \n",
    "- `leaky_relu_derivative`: shape `(4,)`\n",
    "- `error * leaky_relu_derivative`: shape `(4,)` (element-wise)\n",
    "- `x`: shape `(4, 5)`\n",
    "- `np.dot(..., x)`: shape `(5,)` - perfect! One gradient per weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42911c80-5458-426a-8f90-fb8ffa2cbdd9",
   "metadata": {},
   "source": [
    "## Progress Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b69b4-e347-48e5-891a-22fe66254891",
   "metadata": {},
   "source": [
    "You've successfully:\n",
    "- âœ… Implemented a single neuron with Leaky ReLU activation\n",
    "- âœ… Derived and implemented backpropagation gradients\n",
    "- âœ… Created a working gradient computation for batch inputs\n",
    "\n",
    "**Major Conceptual Milestone Achieved**\n",
    "\n",
    "You've now bridged the gap from theoretical understanding to practical implementation of neural network fundamentals. Your gradient computation is mathematically sound and efficiently vectorized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec76bce-dce3-41ef-849b-e1f8f95b537e",
   "metadata": {},
   "source": [
    "## Ready for the Next Step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b5865-50e4-4e4a-8ccf-67902d32245d",
   "metadata": {},
   "source": [
    "Your gradient computation is working! The natural progression is:\n",
    "1. **Weight update**: Use these gradients to actually train the neuron\n",
    "2. **Training loop**: Implement gradient descent\n",
    "3. **Test on real data**: See if it can learn a simple function\n",
    "\n",
    "**What would you like to tackle next - the weight update or creating a training loop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8846046-0d51-4893-a9a3-3664ae4d9693",
   "metadata": {},
   "source": [
    "# <b>F.2</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fbc7a-97f9-4af9-86f9-7d126eb3660c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a7e10b2-5679-4b0c-8328-3e2b22996868",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb45c5bf-2616-4de5-935f-e2cb97827b1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b430f080-1a8e-45ad-8ab6-9c74d3569ab8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e237b871-7a0d-4e8d-a5db-b15b74b4d22f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be4b250d-8300-472b-933d-bed4b0c26cca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1cfb6d7-2ea2-4bbc-b199-f73bb589c253",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4446bb18-4532-4fce-9108-82e049bfa896",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9c49f37-db7c-484c-a223-b67d54ff9467",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_learning",
   "language": "python",
   "name": "pytorch_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
